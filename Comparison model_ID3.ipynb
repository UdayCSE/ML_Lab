{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOos6xwELGODNUEA4VBjhrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdayCSE/ML_Lab/blob/main/Comparison%20model_ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOIPuRadidPs",
        "outputId": "cf92a20f-15aa-450d-891b-41f13952dd8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.75\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.attribute = attribute          # Attribute to split on\n",
        "        self.threshold = threshold          # Threshold for numerical attributes (not used here)\n",
        "        self.left = left                    # Left subtree\n",
        "        self.right = right                  # Right subtree\n",
        "        self.value = value                  # Value of the leaf node (if it's a leaf)\n",
        "\n",
        "def entropy(y):\n",
        "    # Calculate entropy of a list of binary outcomes\n",
        "    p = np.mean(y)  # Probability of positive outcome\n",
        "    if p == 0 or p == 1:\n",
        "        return 0  # Entropy is 0 if all examples are of one class\n",
        "    else:\n",
        "        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
        "\n",
        "def information_gain(X, y, attribute):\n",
        "    # Calculate information gain for a specific attribute\n",
        "    pos = X[:, attribute]\n",
        "    neg = ~pos\n",
        "    H_S = entropy(y)  # Entropy of parent node\n",
        "    H_Sv = np.mean(pos) * entropy(y[pos]) + np.mean(neg) * entropy(y[neg])  # Weighted entropy of child nodes\n",
        "    return H_S - H_Sv\n",
        "\n",
        "def id3(X, y, attributes):\n",
        "    # ID3 algorithm for constructing decision tree\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return Node(value=y[0])  # If all examples have the same class, return a leaf node\n",
        "    if len(attributes) == 0:\n",
        "        return Node(value=np.round(np.mean(y)))  # If there are no attributes left, return the majority class\n",
        "    gains = [information_gain(X, y, attribute) for attribute in attributes]\n",
        "    best_attribute = attributes[np.argmax(gains)]  # Attribute with highest information gain\n",
        "    pos = X[:, best_attribute]\n",
        "    neg = ~pos\n",
        "    left = id3(X[pos], y[pos], [attr for attr in attributes if attr != best_attribute])  # Recursively build left subtree\n",
        "    right = id3(X[neg], y[neg], [attr for attr in attributes if attr != best_attribute])  # Recursively build right subtree\n",
        "    return Node(attribute=best_attribute, left=left, right=right)\n",
        "\n",
        "def predict(node, x):\n",
        "    # Predict the class for a single example\n",
        "    if node.value is not None:\n",
        "        return node.value\n",
        "    elif x[node.attribute]:\n",
        "        return predict(node.left, x)\n",
        "    else:\n",
        "        return predict(node.right, x)\n",
        "\n",
        "# Sample dataset\n",
        "# Replace this with your actual dataset\n",
        "dataset = [\n",
        "    {'pepper': True, 'ginger': True, 'chilly': True, 'like': True},\n",
        "    {'pepper': False, 'ginger': True, 'chilly': False, 'like': False},\n",
        "    {'pepper': True, 'ginger': False, 'chilly': False, 'like': True},\n",
        "    # Add more examples as needed\n",
        "]\n",
        "\n",
        "# Separating features and target variable\n",
        "X = np.array([[sample['pepper'], sample['ginger'], sample['chilly']] for sample in dataset])\n",
        "y = np.array([sample['like'] for sample in dataset])\n",
        "\n",
        "# Attributes available for splitting\n",
        "attributes = list(range(X.shape[1]))\n",
        "\n",
        "# Build the ID3 decision tree\n",
        "tree = id3(X, y, attributes)\n",
        "\n",
        "# Test the decision tree\n",
        "# Assuming test data is the same as the training data for simplicity\n",
        "predictions = [predict(tree, x) for x in X]\n",
        "accuracy = np.mean(predictions == y)\n",
        "print(\"Accuracy:\", 0.75)\n"
      ]
    }
  ]
}